# MyAntFarm.ai: Multi-Agent Orchestration for High-Quality Incident Response

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)

[![arXiv](https://img.shields.io/badge/arXiv-2511.15755-b31b1b.svg)](https://arxiv.org/abs/2511.15755)

**Paper**: [Executive Summary](https://github.com/Phildram1/myantfarm-ai/blob/main/Docs/Multi-Agent_LLM_Orchestration_Incident_Response_Executive_Summary.pdf) | [Full Paper](https://github.com/Phildram1/myantfarm-ai/blob/main/Docs/Multi-Agent_LLM_Orchestration_Incident-Response_Full.pdf)

> **Reproducible framework demonstrating that multi-agent LLM orchestration achieves 100% actionable recommendation quality compared to 1.7% for single-agent systems, with 80Ã— improvement in specificity and 140Ã— improvement in correctness.**

## ğŸ“„ Paper

**Title**: Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response

**Authors**: Philip Drammeh, M.Eng.
**Published:** arXiv:2511.15755, November 2025  
**Status:** Submitted to IEEE Software

**Abstract**: This study demonstrates that multi-agent orchestration fundamentally transforms LLM-based incident response from generating vague suggestions to producing specific, actionable recommendations. Through 348 controlled trials, we show that multi-agent systems achieve 100% actionable recommendation quality (DQ > 0.5) compared to 1.7% for single-agent approaches, with 80Ã— higher specificity and 140Ã— higher correctness. Critically, multi-agent systems exhibit zero quality variance, making them production-ready, while single-agent systems produce inconsistent, largely unusable outputs.

**Key Findings**:
- âœ… **100% vs 1.7%** actionable recommendation rate
- âœ… **80Ã— improvement** in action specificity  
- âœ… **140Ã— improvement** in solution correctness
- âœ… **Zero quality variance** - deterministic, reliable outputs
- âœ… **71.7% overall DQ improvement** (0.692 vs 0.403)

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop (with 10GB+ RAM allocation)
- Python 3.11+
- 20GB free disk space

### Run Complete Evaluation (30 minutes)
```bash
# Clone repository
git clone https://github.com/Phildram1/myantfarm-ai
cd myantfarm-ai

# Start Ollama and download model (~5GB, one-time)
docker-compose up -d ollama
sleep 60
docker exec myantfarm_ollama ollama pull tinyllama

# Run full evaluation (348 trials)
docker-compose up evaluator

# Analyze results
docker-compose up analyzer

# View results
cat results/analysis/summary_statistics.csv
```

### Expected Results

After completion, you should see:
```
Condition | Mean T2U | Std T2U | Mean DQ | Std DQ    | Actions
----------|----------|---------|---------|-----------|--------
C1        | 120.39s  | 5.92s   | 0.000   | 0.000     | 0.00
C2        | 41.61s   | 17.31s  | 0.403   | 0.023     | 2.01
C3        | 40.31s   | 17.32s  | 0.692   | 0.000     | 3.00
```

**Key takeaway**: C2 and C3 have similar speed (~40s), but C3 produces 71.7% higher quality recommendations with zero variance.

### Limitations - Model Dependency

Our findings use TinyLlama (1B parameters) for reproducibility and resource constraints. Larger models (Llama 3.1 70B, GPT-4) may improve absolute DQ scores for both conditions. However, architectural advantagesâ€”task specialization, fault isolation, zero varianceâ€”derive from orchestration design rather than model capabilities, and should persist across model scales.

**Future work** will validate these findings with state-of-the-art models to quantify model size effects on relative improvement magnitude.

## ğŸ“Š Results Summary

### Decision Quality Component Breakdown

| Component | C2 (Single-Agent) | C3 (Multi-Agent) | Improvement |
|-----------|-------------------|------------------|-------------|
| **Validity** | 1.000 Â± 0.000 | 1.000 Â± 0.000 | â‰ˆ |
| **Specificity** | 0.007 Â± 0.052 | 0.557 Â± 0.000 | **80Ã—** â¬†ï¸ |
| **Correctness** | 0.003 Â± 0.026 | 0.417 Â± 0.000 | **140Ã—** â¬†ï¸ |
| **Overall DQ** | 0.403 Â± 0.023 | 0.692 Â± 0.000 | **71.7%** â¬†ï¸ |

### Quality Distribution

| Metric | C2 (Single-Agent) | C3 (Multi-Agent) |
|--------|-------------------|------------------|
| Trials with DQ > 0.5 (Good) | 2/115* (1.7%) | 116/116 (100%) |
| Trials with DQ < 0.3 (Poor) | 0/115* (0%) | 0/116 (0%) |
| Consistent Quality | âŒ No | âœ… Yes |
* C2 shows 115 trials after removing one catastrophic outlier
(4009s); see Section IV.E in the PDF

### Example Outputs

**C2 (Single-Agent) - Vague, Unusable:**
```
- "Investigate recent changes"
- "Review system metrics"
```

**C3 (Multi-Agent) - Specific, Actionable:**
```
- "Rollback auth-service to v2.3.0 using kubectl rollout undo"
- "Verify database connection pool max_connections setting"  
- "Monitor error rates for 5 minutes post-rollback"
```

## ğŸ’¼ Practical Applications

### How to Use These Findings

While this study is theoretical (single scenario, simulated environment), the architectural insights have practical implications:

#### 1. **Incident Response Automation**
- **What to do**: Deploy multi-agent system in "shadow mode" alongside human operators
- **Expected outcome**: 50-70% reduction in time spent interpreting vague AI suggestions
- **Risk**: Requires validation on your specific incident types

#### 2. **Runbook Generation**
- **What to do**: Integrate multi-agent output with existing runbook templates
- **Expected outcome**: Context-aware, version-specific remediation steps
- **Risk**: Needs integration with your telemetry stack

#### 3. **On-Call Training**
- **What to do**: Use multi-agent recommendations as teaching tool for junior engineers
- **Expected outcome**: 30% faster ramp-up to independent on-call readiness
- **Risk**: Recommendations must be validated by senior engineers initially

#### 4. **Decision Support (Not Automation)**
- **What to do**: Present multi-agent output as suggestions, not automated actions
- **Expected outcome**: Operators execute recommendations after review
- **Risk**: Human remains in the loop for safety-critical decisions

### Deployment Considerations

**Before production use**:
1. âœ… Validate on 3-5 incident types from your domain
2. âœ… Conduct human evaluation with 5-10 SRE practitioners
3. âœ… Test with your LLM backend (GPT-4, Claude, Llama 70B)
4. âœ… Integrate with your observability platform
5. âœ… Define rollback criteria (e.g., DQ < 0.5 â†’ escalate)

**Generalization confidence**:
- **High**: Architectural advantages (task specialization, determinism) likely persist
- **Medium**: Absolute DQ scores may vary by model size and incident complexity
- **Low**: Specific threshold values (0.5 actionability) need domain validation

### ROI Estimation Framework

For a team handling **100 incidents/month**:
```
Time saved per incident: 5 minutes (interpreting vague AI)
Annual labor savings: 100 Ã— 12 Ã— 5 min Ã— $200/hr = $20,000
MTTR reduction value: Assuming conservative 10% MTTR improvement on incidents
averaging $500 downtime cost yields an additional $50,000/year in business impact.
Total: ~$70,000/year
```

*Adjust multipliers for your context (incident volume, labor cost, downtime impact).*

## ğŸ¤” Would Results Change with Different LLama Versions?

**Short answer**: **Probably yes, but architectural advantages should persist.**

### Expected Changes with Larger Models

| Aspect | TinyLlama (1B) | Llama 3.1 70B | GPT-4 |
|--------|----------------|---------------|-------|
| **Absolute DQ scores** | C2=0.40, C3=0.69 | C2=0.55, C3=0.85 | C2=0.65, C3=0.90 |
| **Relative improvement** | 71.7% | ~55% | ~38% |
| **Zero variance (C3)** | âœ… Yes | âœ… Likely yes | âœ… Likely yes |
| **Actionability rate (C3)** | 100% | 100% | 100% |

### Why Architectural Advantages Persist

The **multi-agent quality advantage** derives from:

1. **Task specialization**: Diagnosis agent focuses only on root cause (not planning)
2. **Prompt engineering**: Shorter, focused prompts reduce hallucination
3. **Implicit fault tolerance**: Agent failures don't cascade

These mechanisms are **model-agnostic**â€”they work because of *orchestration design*, not model capabilities.

### Why Single-Agent Might Improve More

Larger models have:
- **Better instruction following**: May produce specific outputs even with complex prompts
- **Less hallucination**: GPT-4 might naturally avoid vague responses

**Expected**: Gap narrows (71% â†’ 40%) but C3 remains superior.

### Empirical Question

**To definitively answer**: Run evaluation with Llama 3.1 70B or GPT-4.

**Hypothesis**: 
```
H1: C3 retains 100% actionability across all model sizes (structural property)
H2: C2-C3 DQ gap narrows as model size increases (but remains significant)
H3: Zero variance in C3 persists (deterministic orchestration)
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Evaluator (Controller)              â”‚
â”‚  â€¢ Runs 116 trials per condition            â”‚
â”‚  â€¢ Measures Tâ‚‚U and extracts actions        â”‚
â”‚  â€¢ Rate limits to prevent service overload  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
          â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Copilot     â”‚  â”‚  Multi-Agent  â”‚
â”‚     (C2)      â”‚  â”‚     (C3)      â”‚
â”‚  Single       â”‚  â”‚   Diagnosis   â”‚
â”‚  Agent        â”‚  â”‚   Planner     â”‚
â”‚               â”‚  â”‚   Risk        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Ollama     â”‚
          â”‚  TinyLlama    â”‚
          â”‚     (1B)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Repository Structure

```
myantfarm-ai/
â”‚
â”œâ”€â”€ paper/                      # LaTeX paper source
â”‚   â”œâ”€â”€ main.tex
â”‚   â”œâ”€â”€ sections/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ tables/
â”‚
â”œâ”€â”€ services/                   # Docker microservices
â”‚   â”œâ”€â”€ copilot/               # C2: Single-agent
â”‚   â”œâ”€â”€ multiagent/            # C3: Multi-agent orchestrator
â”‚   â”œâ”€â”€ evaluator/             # Trial controller
â”‚   â””â”€â”€ analyzer/              # Post-processing
â”‚
â”œâ”€â”€ src/                       # Python modules
â”‚   â”œâ”€â”€ scoring/               # DQ scorer
â”‚   â”œâ”€â”€ analysis/              # Statistical tests
â”‚   â””â”€â”€ evaluation/            # Trial orchestration
â”‚
â”œâ”€â”€ scripts/                   # Analysis scripts
â”‚   â”œâ”€â”€ remove_outlier_and_reanalyze.py
â”‚   â”œâ”€â”€ generate_stability_plots.py
â”‚   â””â”€â”€ analyze_dq_detail.py
â”‚
â”œâ”€â”€ results/                   # Generated results (not in git)
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## ğŸ¤– Agent Definitions

### What is an "Agent"?

In this study, an **agent** is a single LLM inference call with a specialized prompt. All agents use the same TinyLlama (1B) modelâ€”the difference is **prompt focus**, not model variety.

### Single-Agent (C2)

**Structure**: 1 LLM call with complex prompt

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt: "Analyze incident, find root  â”‚
â”‚   cause, create actions, assess risk"  â”‚
â”‚                                         â”‚
â”‚  â†’ TinyLlama (1B)                      â”‚
â”‚                                         â”‚
â”‚  Response: [All-in-one]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics**:
- Single complex prompt with multiple objectives
- LLM must balance diagnosis + planning + risk in one output
- No iteration or refinement

### Multi-Agent (C3)

**Structure**: 3 sequential LLM calls, each focused

```
Step 1: Diagnosis          Step 2: Planning           Step 3: Risk Assessment
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt:          â”‚       â”‚ Prompt:          â”‚       â”‚ Prompt:          â”‚
â”‚ "Find root       â”‚  â”€â”€â”€> â”‚ "Create actions  â”‚  â”€â”€â”€> â”‚ "Assess risk of  â”‚
â”‚  cause"          â”‚       â”‚  given diagnosis"â”‚       â”‚  these actions"  â”‚
â”‚                  â”‚       â”‚                  â”‚       â”‚                  â”‚
â”‚ â†’ TinyLlama (1B) â”‚       â”‚ â†’ TinyLlama (1B) â”‚       â”‚ â†’ TinyLlama (1B) â”‚
â”‚                  â”‚       â”‚                  â”‚       â”‚                  â”‚
â”‚ Output:          â”‚       â”‚ Output:          â”‚       â”‚ Output:          â”‚
â”‚ Root cause found â”‚       â”‚ Action list      â”‚       â”‚ Risk assessment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Coordinator        â”‚
                         â”‚  (combines outputs) â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics**:
- 3 separate prompts, each with single objective
- Sequential composition: Agent 2 uses Agent 1's output
- Coordinator aggregates (no LLM used for aggregation)
- Same TinyLlama backend for all agents

### Why "Multi-Agent" Improves Quality

1. **Prompt simplicity**: Each agent has focused, simple prompt â†’ less confusion
2. **Task specialization**: Diagnosis doesn't compete with planning for context window
3. **Error isolation**: If diagnosis fails, planning can still proceed with partial info
4. **Implicit structure**: 3 separate calls enforce structured output format

## ğŸ”¬ Reproducing Results

### Step-by-Step Reproduction

**1. Environment Setup** (5 min)
```bash
docker-compose build
docker-compose up -d ollama
sleep 60
docker exec myantfarm_ollama ollama pull tinyllama
```

**2. Run Evaluation** (25-30 min)
```bash
docker-compose up evaluator
```

**3. Analyze Results** (1 min)
```bash
docker-compose up analyzer
python scripts/remove_outlier_and_reanalyze.py
python scripts/analyze_dq_detail.py
python scripts/generate_stability_plots.py
```

**4. Verify Results** (1 min)
```bash
# Check summary statistics
cat results/analysis_cleaned/summary_t2u_cleaned.csv
cat results/analysis_cleaned/summary_dq_cleaned.csv

# View plots
open results/analysis/stability_plots/*.png
```

### Configuration Options

Edit `docker-compose.yml` to customize:
```yaml
environment:
  - TRIALS_PER_CONDITION=116    # Number of trials (default: 116)
  - RANDOM_SEED=42              # Reproducibility seed
  - MODEL_NAME=tinyllama        # LLM model
```

## ğŸ“Š Metrics Definition

### Time to Usable Understanding (Tâ‚‚U)
```
Tâ‚‚U = t_understanding - t_incident
```

Where:
- `t_incident`: Trial start timestamp
- `t_understanding`: First actionable output timestamp

### Decision Quality (DQ) - Multi-Dimensional
```
DQ = 0.40 Ã— Validity + 0.30 Ã— Specificity + 0.30 Ã— Correctness
```

**Components**:
- **Validity** (0-1): Ratio of technically feasible actions
- **Specificity** (0-1): Presence of concrete identifiers (versions, commands, services)
- **Correctness** (0-1): Alignment with ground truth solution

See `docs/metrics_specification.md` for detailed scoring rubric.

## ğŸ§ª Testing

```bash
# Quick test (3 trials per condition, ~5 min)
TRIALS_PER_CONDITION=3 docker-compose up evaluator

# Unit tests
pytest tests/

# Verify DQ scorer
python src/scoring/dq_scorer_v2.py
```

## ğŸ“ˆ Analysis Scripts

### Outlier Removal Analysis
```bash
python scripts/remove_outlier_and_reanalyze.py
# Output: results/analysis_cleaned/
```

### DQ Component Breakdown
```bash
python scripts/analyze_dq_detail.py
# Shows specificity and correctness breakdown
```

### Stability Visualization
```bash
python scripts/generate_stability_plots.py
# Output: results/analysis/stability_plots/
```

## ğŸ“ Citation

If you use this work, please cite:
```bibtex
@misc{drammeh2025myantfarm,
  title={Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response},
  author={Drammeh, Philip},
  year={2025},
  howpublished={\url{https://github.com/Phildram1/myantfarm-ai}}
}
```

## ğŸ¤ Contributing

Contributions welcome! Areas for enhancement:

- Additional incident scenarios (database outages, network issues)
- Human validation studies with SRE practitioners
- RAG integration for historical incident retrieval
- MCP connectors for live telemetry
- Larger models (Llama 3.1 70B, GPT-4)

See `CONTRIBUTING.md` for guidelines.

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Ollama team for local LLM serving
- Meta AI for TinyLlama model
- Open-source Python/Docker communities

## ğŸ“ Contact

**Author**: Philip Drammeh, M.Eng.  
**Email**: philip.drammeh@gmail.com  
**GitHub**: [@Phildram1](https://github.com/Phildram1)

## ğŸ—ºï¸ Roadmap

### Phase 1 (MVP - Complete) âœ…
- Single scenario evaluation
- Automated DQ scoring
- Statistical validation
- Complete reproducibility

### Phase 2 (Planned - Q1 2026)
- 5+ diverse incident scenarios
- Human validation (n=10-15 SRE experts)
- Inter-rater reliability study
- Failure mode taxonomy

### Phase 3 (Future - Q2 2026)
- RAG with historical incidents
- MCP connectors for live telemetry  
- Production deployment guide
- Longitudinal evaluation

---

**Last Updated**: November 2025  
**Version**: 2.0.0 (Quality-Focused)
