# MyAntFarm.ai - Multi-Agent LLM Orchestration for Incident Response

[![arXiv](https://img.shields.io/badge/arXiv-2511.15755-b31b1b.svg)](https://arxiv.org/abs/2511.15755)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Publication**: "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response"  
**Author**: Philip Drammeh, M.Eng.  
**arXiv**: [2511.15755v2](https://arxiv.org/abs/2511.15755v2)

## Overview

MyAntFarm.ai is a reproducible experimental framework demonstrating that multi-agent LLM orchestration achieves **100% actionable recommendation quality** compared to 1.7% for single-agent approaches in incident response scenarios.

### Key Findings

- **100% actionability** (multi-agent) vs 1.7% (single-agent)
- **80Ã— improvement** in action specificity
- **140Ã— improvement** in solution correctness
- **Zero quality variance** across all trials (production-ready)
- **Similar latency** (~40s) - quality, not speed, is the differentiator

## What's New in v2 (January 2026)

**Version 2 updates** (no changes to experimental results):
- âœ… Expanded limitations section with deeper analysis
- âœ… Added comprehensive threats to validity analysis
- âœ… Clarified "agent" definition (vs autonomous agent frameworks)
- âœ… Added reproducibility notes for researchers
- âœ… Updated Phase 2 timeline with current status (experiments in progress)
- âœ… Updated model references to latest versions (GPT-5.2, Claude Sonnet 4.5, Llama 3.3 70B)
- âœ… Strengthened practical implications for AIOps tool builders

---

## ğŸš€ Quick Start (Verified Reproducible)

### Prerequisites
- **Docker Desktop** (with 8GB+ RAM allocation)
- **Python 3.11+** (for local analysis scripts only)
- **20GB free disk space** (includes Ollama model)
- **40 minutes** for complete evaluation

### Step 1: Clone and Setup (5 minutes)

```bash
# Clone repository
git clone https://github.com/Phildram1/myantfarm-ai.git
cd myantfarm-ai

# Start services and build containers
docker-compose up -d

# Wait for services to initialize
sleep 60
```

### Step 2: Load Ollama Model (2-3 minutes, one-time)

**CRITICAL**: The TinyLlama model must be loaded before running trials.

```bash
# Pull TinyLlama model (~637MB download)
docker exec -it myantfarm_ollama ollama pull tinyllama

# Verify model is loaded
docker exec -it myantfarm_ollama ollama list
# Should show: tinyllama:latest    ...    637 MB
```

### Step 3: Run Full Evaluation (30-40 minutes)

```bash
# Run 348 trials (116 per condition: C1, C2, C3)
docker exec -it myantfarm_evaluator python run_evaluation.py

# Monitor progress (optional, in another terminal)
docker logs -f myantfarm_evaluator
```

**Expected output**:
```
Running C1 (Baseline) trials...
âœ“ C1 complete: 116 trials

Running C2 (Single-Agent) trials...
  (Rate limited: ~6 seconds per trial)
âœ“ C2 complete: 116 trials

Running C3 (Multi-Agent) trials...
  (Rate limited: ~6 seconds per trial)
âœ“ C3 complete: 116 trials

âœ… Evaluation complete!
Total trials: 348
Results saved to: /app/results
```

### Step 4: Score Trials (30 seconds)

```bash
# Copy scoring code to container
docker cp src myantfarm_evaluator:/app/

# Run manual scoring script
docker exec -it myantfarm_evaluator python -c "
import json, sys
sys.path.insert(0, '/app/src')
from scoring.dq_scorer_v2 import DQScorer

with open('/app/results/all_trials.json', 'r') as f:
    data = json.load(f)

ground_truth = 'rollback auth-service deployment to v2.3.0 verify database connection pool'
scorer = DQScorer(ground_truth)

for trial in data['trials']:
    if trial['condition'] in ['C2', 'C3']:
        actions = trial.get('actions', [])
        if actions:
            result = scorer.score_trial(actions)
            trial['dq_score'] = result['dq']
        else:
            trial['dq_score'] = 0.0
    else:
        trial['dq_score'] = 0.0

with open('/app/results/all_trials.json', 'w') as f:
    json.dump(data, f, indent=2)

print('Scoring complete!')
"
```

### Step 5: Analyze Results (10 seconds)

```bash
# Install pandas locally (if needed)
pip install pandas numpy scipy

# Run analysis
python analyze_results.py
```

**Expected output**:
```
======================================================================
RESULTS:
======================================================================

C2 (Single-Agent):
  Mean DQ: 0.403
  Std DQ: 0.033
  Actionable: 1/116

C3 (Multi-Agent):
  Mean DQ: 0.692
  Std DQ: 0.000
  Actionable: 116/116
======================================================================
```

---

## âš ï¸ Important: Running the Evaluator

**The evaluator runs INSIDE a Docker container, not as a standalone Python script.**

### âŒ Common Mistakes (Don't Do This)

```bash
# âŒ WRONG - This directory doesn't exist!
python src/evaluator/run_trials.py

# âŒ WRONG - Needs Docker container environment
python services/evaluator/run_evaluation.py
```

### âœ… Correct Methods

**Method 1: Direct Docker Command (Recommended)**
```bash
docker exec -it myantfarm_evaluator python run_evaluation.py
```

**Method 2: Quick Test (9 trials, ~5 minutes)**
```bash
docker exec -it myantfarm_evaluator python -c "
import os
os.environ['TRIALS_PER_CONDITION'] = '3'
exec(open('run_evaluation.py').read())
"
```

### Why Docker is Required

The evaluator needs:
- Access to other services (Ollama, Copilot, MultiAgent) via Docker network
- Shared Docker volumes for results storage
- Container-specific environment variables and configuration
- Service discovery via Docker Compose networking

---

## ğŸ—ï¸ Architecture

MyAntFarm.ai consists of five containerized microservices orchestrated via Docker Compose:

1. **LLM Backend**: Ollama (v0.1.32) serving TinyLlama (1B parameters, 4-bit quantized)
2. **Copilot (C2)**: FastAPI service implementing single-agent summarization
3. **MultiAgent (C3)**: Coordinator dispatching to specialized agents:
   - Diagnosis Specialist
   - Remediation Planner
   - Risk Assessor
4. **Evaluator**: Controller executing trials with rate limiting (10 calls/min)
5. **Analyzer**: Statistical analysis pipeline

All services share persistent volumes ensuring deterministic reproduction across environments.

---

## ğŸ“ Repository Structure

```
myantfarm-ai/
â”‚
â”œâ”€â”€ services/                          # Docker microservices (5 services from paper)
â”‚   â”œâ”€â”€ ollama/                        # Service 1: LLM Backend
â”‚   â”œâ”€â”€ copilot/                       # Service 2: Single-agent (C2)
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ multiagent/                    # Service 3: Multi-agent (C3)
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ evaluator/                     # Service 4: Trial controller
â”‚   â”‚   â””â”€â”€ run_evaluation.py          # â† Main evaluation script (run via Docker!)
â”‚   â””â”€â”€ analyzer/                      # Service 5: Post-processing
â”‚       â””â”€â”€ compute_metrics.py
â”‚
â”œâ”€â”€ src/                               # Shared Python libraries (NOT services!)
â”‚   â”œâ”€â”€ scoring/                       # DQ scorer implementation
â”‚   â”‚   â””â”€â”€ dq_scorer_v2.py
â”‚   â”œâ”€â”€ analysis/                      # Statistical analysis utilities
â”‚   â”‚   â””â”€â”€ statistical_tests.py
â”‚   â”œâ”€â”€ evaluation/                    # Trial rescoring utilities
â”‚   â”‚   â””â”€â”€ rescore_all_trials.py
â”‚   â””â”€â”€ utils/                         # Common utilities
â”‚       â”œâ”€â”€ llm_interface.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ scenarios/                     # Incident scenarios (JSON)
â”‚   â”‚   â””â”€â”€ auth_service_outage.json
â”‚   â””â”€â”€ results/                       # Trial outputs (gitignored)
â”‚
â”œâ”€â”€ scripts/                           # Convenience scripts
â”œâ”€â”€ tests/                             # Unit tests
â”œâ”€â”€ Docs/                              # Documentation
â”œâ”€â”€ paper/                             # LaTeX paper source
â”œâ”€â”€ docker-compose.yml                 # Service orchestration
â”œâ”€â”€ analyze_results.py                 # Analysis script (run on host)
â””â”€â”€ README.md
```

### Key Files Reference

| Purpose | Location | How to Run |
|---------|----------|------------|
| **Run 348 trials** | `services/evaluator/run_evaluation.py` | `docker exec -it myantfarm_evaluator python run_evaluation.py` |
| **DQ scoring logic** | `src/scoring/dq_scorer_v2.py` | Used by evaluator (imported) |
| **Statistical analysis** | `src/analysis/statistical_tests.py` | Used by analyzer |
| **Analyze results** | `analyze_results.py` (root) | `python analyze_results.py` (on host) |
| **Docker orchestration** | `docker-compose.yml` | `docker-compose up -d` |
| **Main scenario** | `data/scenarios/auth_service_outage.json` | Used by evaluator |

### Why This Structure?

- **`services/`** = Containerized microservices that run independently
- **`src/`** = Shared Python libraries imported by services (not containers)
- **Separation of concerns** = Services run in Docker, libraries are shared code
- **Reproducibility** = Each service has its own Dockerfile and dependencies
- **Clarity** = Matches "five containerized microservices" in paper Section III.A

---

## ğŸ“Š Decision Quality (DQ) Metric

Novel evaluation framework measuring:
- **Validity** (40%): Technical feasibility of recommended actions
- **Specificity** (30%): Presence of concrete identifiers (versions, commands, service names)
- **Correctness** (30%): Alignment with ground truth solutions

**Actionability threshold**: DQ > 0.5

---

## âœ… Verification

Your reproduction is successful if you see:

| Metric | Expected | Acceptable Range |
|--------|----------|------------------|
| **C2 Mean DQ** | 0.403 | 0.393 - 0.413 |
| **C2 Std DQ** | 0.023-0.033 | 0.020 - 0.040 |
| **C2 Actionable** | 1-2/116 | 0-3/116 (0.9-2.6%) |
| **C3 Mean DQ** | 0.692 | 0.682 - 0.702 |
| **C3 Std DQ** | 0.000 | **Must be 0.000** |
| **C3 Actionable** | 116/116 | **Must be 100%** |

**Key validation points**:
- âœ… C3 Mean DQ is ~70% higher than C2 (0.692 vs 0.403)
- âœ… C3 has **zero variance** (all trials produce identical quality)
- âœ… C3 achieves 100% actionability vs ~1% for C2

---

## ğŸ”§ Troubleshooting

### Issue: "404 Not Found" in copilot logs

**Cause**: Ollama model not loaded

**Fix**:
```bash
docker exec -it myantfarm_ollama ollama pull tinyllama
docker exec -it myantfarm_ollama ollama list  # Verify
docker-compose restart copilot
```

### Issue: All DQ scores are 0

**Cause**: Scoring code not in container

**Fix**:
```bash
docker cp src myantfarm_evaluator:/app/
# Re-run scoring step (Step 4 above)
```

### Issue: Copilot timeouts during trials

**Cause**: Timeout too short for slow machines

**Fix**: Edit `services/copilot/main.py`:
```python
# Line 58: Increase timeout if needed
async with httpx.AsyncClient(timeout=300.0) as client:  # Default: 300s
```

### Issue: C2 Mean DQ is 0.538 (not 0.403)

**Cause**: All C2 trials used fallback (circuit breaker opened)

**Symptoms**:
- All C2 trials have identical actions
- C2 Std DQ = 0.0
- Copilot logs show "Using fallback response"

**Fix**:
```bash
# Verify Ollama model is loaded
docker exec -it myantfarm_ollama ollama list

# Check copilot can reach Ollama
docker logs myantfarm_copilot | grep "404"

# If 404s present, restart with model loaded
docker-compose down
docker-compose up -d
docker exec -it myantfarm_ollama ollama pull tinyllama
sleep 60
# Re-run evaluation
```

---

## ğŸ“ˆ Results Summary

| Condition | Mean T2U (s) | Mean DQ | Actionable Rate |
|-----------|--------------|---------|-----------------|
| C1 (Baseline) | 120.39 | 0.000 | N/A |
| C2 (Single-Agent) | 41.61 | 0.403 | 1.7% |
| C3 (Multi-Agent) | 40.31 | **0.692** | **100%** |

**Statistical significance**: All comparisons p < 0.001, Cohen's d > 18

### Decision Quality Component Breakdown

| Component | C2 (Single-Agent) | C3 (Multi-Agent) | Improvement |
|-----------|-------------------|------------------|-------------|
| **Validity** | 1.000 Â± 0.000 | 1.000 Â± 0.000 | â‰ˆ |
| **Specificity** | 0.007 Â± 0.052 | 0.557 Â± 0.000 | **80Ã—** â¬†ï¸ |
| **Correctness** | 0.003 Â± 0.026 | 0.417 Â± 0.000 | **140Ã—** â¬†ï¸ |
| **Overall DQ** | 0.403 Â± 0.023 | 0.692 Â± 0.000 | **71.7%** â¬†ï¸ |

---

## ğŸ§ª Reproducibility

All experiments are fully deterministic:
- **Random seed**: 42 (set in evaluator configuration)
- **LLM temperature**: 0.7 (fixed across all trials)
- **Model**: TinyLlama 1.1B parameters, 4-bit quantization
- **Ollama version**: 0.1.32
- **Expected runtime**: 25-30 minutes for full 348-trial evaluation on 16GB RAM system with CPU inference

### Common Issues

1. **Ollama connection errors**: Ensure service running on port 11434
2. **Docker network conflicts**: Use `docker-compose down -v` to reset
3. **Memory exhaustion**: Reduce concurrent trials in evaluator config

---

## ğŸš€ Phase 2 Status (Q1-Q2 2026)

Currently in progress:
- âœ… **Multi-scenario validation**: 5 diverse incident types
- âœ… **Model scaling study**: Llama 3.3 70B, GPT-5.2, Claude Sonnet 4.5
- ğŸ”„ **Human validation study**: 10-15 SRE practitioners (recruitment underway)
- ğŸ“… **RAG integration**: Q2 2026
- ğŸ“… **MCP integration**: Q2 2026

---

## ğŸ’¼ Practical Applications

1. **Incident Response Automation**: Deploy in shadow mode alongside human operators
2. **Runbook Generation**: Generate version-specific remediation steps
3. **Junior Engineer Onboarding**: Teaching tool validated by senior engineers
4. **Decision Support**: Human-in-the-loop recommendations with confidence scores

### Deployment Checklist

Before production use:
- [ ] Validate on 3-5 incident types from your domain
- [ ] Conduct human evaluation with 5-10 SRE practitioners
- [ ] Test with your LLM backend (GPT-5.2, Claude Sonnet 4.5, Llama 3.3 70B)
- [ ] Integrate with observability platform (Datadog, Splunk, Prometheus)
- [ ] Define rollback criteria (e.g., DQ < 0.5 â†’ escalate to human)

---

## ğŸ“ Citation

```bibtex
@article{drammeh2025multiagent,
  title={Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response},
  author={Drammeh, Philip},
  journal={arXiv preprint arXiv:2511.15755},
  year={2025}
}
```

---

## ğŸ“ License

MIT License - see LICENSE file for details

---

## ğŸ“ Contact

**Philip Drammeh, M.Eng.**  
Email: philip.drammeh@gmail.com  
GitHub: [@Phildram1](https://github.com/Phildram1)  
LinkedIn: [Philip Drammeh](https://www.linkedin.com/in/philip-drammeh/)

For reproduction support or questions about the research, please open an issue or email directly.

---

## ğŸ™ Acknowledgments

Thanks to the open-source communities behind:
- Ollama team for local LLM serving
- Meta AI for TinyLlama model
- Python/Docker ecosystems
- SRE practitioners who provided domain insights

---

**Research conducted independently without institutional affiliation or funding.**
